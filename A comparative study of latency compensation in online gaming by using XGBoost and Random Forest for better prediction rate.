import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_absolute_error
from xgboost import XGBRegressor
from sklearn.ensemble import RandomForestRegressor

# Load dataset (assuming CSV format)
data = pd.read_csv("/content/sample_data/online game.csv")

# Selecting relevant features
features = ["Rank", "Platform", "Year", "Genre", "Publisher", "NA_Sales", "EU_Sales", "JP_Sales", "Other_Sales"]
target = "Global_Sales"

data = data[features + [target]].dropna()

# Encoding categorical variables
label_encoders = {}
for col in ["Platform", "Genre", "Publisher"]:
    label_encoders[col] = LabelEncoder()
    data[col] = label_encoders[col].fit_transform(data[col])

# Splitting dataset
X = data[features]
y = data[target]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Training XGBoost Model
xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
xgb_model.fit(X_train, y_train)
xgb_preds = xgb_model.predict(X_test)

# Training Random Forest Model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
rf_preds = rf_model.predict(X_test)

# Evaluating performance
xgb_mae = mean_absolute_error(y_test, xgb_preds)
rf_mae = mean_absolute_error(y_test, rf_preds)

print(f"XGBoost MAE: {xgb_mae:.4f}")
print(f"Random Forest MAE: {rf_mae:.4f}")

# Conclusion
if xgb_mae < rf_mae:
    print("XGBoost performs better for latency compensation prediction.")
else:
    print("Random Forest performs better for latency compensation prediction.")

XGBoost MAE: 0.0416
Random Forest MAE: 0.0167
Random Forest performs better for latency compensation prediction.
