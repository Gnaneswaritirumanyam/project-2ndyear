# Importing necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Load the dataset
file_path = '/content/sample_data/online games.csv'  # Replace with the path to your dataset
data = pd.read_csv(file_path)

# Handle missing values by filling them with 0 (or use another strategy if needed)
data.fillna(0, inplace=True)

# Feature selection (Dropping non-numeric or irrelevant columns)
X = data[['Rank', 'Year', 'NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales']]
y = data['Global_Sales']

# Ensure there are no NaNs or infinite values
X = X.replace([float('inf'), float('-inf')], 0)
y = y.replace([float('inf'), float('-inf')], 0)

# Split the dataset into training and test sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the Random Forest model
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
rf_pred = rf.predict(X_test)

# Initialize and train the XGBoost model
xgb = XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)
xgb.fit(X_train, y_train)
xgb_pred = xgb.predict(X_test)

# Calculate RÂ² score for accuracy
rf_accuracy = r2_score(y_test, rf_pred) * 100
xgb_accuracy = r2_score(y_test, xgb_pred) * 100

# Display results
print(f"Random Forest Accuracy: {rf_accuracy:.2f}%")
print(f"XGBoost Accuracy: {xgb_accuracy:.2f}%")

# Plot the comparison
models = ['Random Forest', 'XGBoost']
accuracies = [rf_accuracy, xgb_accuracy]

plt.figure(figsize=(8, 5))
plt.bar(models, accuracies, color=['skyblue', 'orange'])
plt.title('Comparison of Prediction Accuracy')
plt.ylabel('Accuracy (%)')
plt.ylim(0, 100)
for i, value in enumerate(accuracies):
    plt.text(i, value + 1, f"{value:.2f}%", ha='center')

plt.show()
